{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a718e020",
   "metadata": {},
   "source": [
    "# Enhancing SQL Queries with Few-Shot Learning via CodeLlama and LangChain\n",
    "\n",
    "## Prerequisites for running this notebook\n",
    "To ensure a smooth execution of this Jupyter notebook, there are a few prerequisites that need to be met. Follow these steps to prepare your environment:\n",
    "\n",
    "**1. Clone the Repository:** \\\n",
    "Start by forking or cloning the code repository from this GitHub link: [CodeLlama-LangChain-MySql] \\(https://github.com/yernenip/CodeLlama-LangChain-MySql)\\\n",
    "\n",
    "**2. Set Up Docker Container with Sakila DB:** \\\n",
    "Get your Docker container running, and ensure that MySQL is installed within it. \\\n",
    "Install the Sakila database within your MySQL instance. Refer to the repository's README for instructions on how to do this. \n",
    "\n",
    "**3. Install Required Python Packages:** \\\n",
    "Open your terminal or command prompt and navigate to the directory where this notebook is located. \n",
    "\n",
    "**Run the following commands to install the necessary Python packages using pip:** \n",
    "\n",
    "`pip install ctransformers  # For base transformers with no GPU acceleration (CPU)` \\\n",
    "`pip install ctransformers[cuda]  # If you have CUDA support, for GPU acceleration` \\\n",
    "`pip install langchain  # For core functionality` \\\n",
    "`pip install sqlalchemy  # Used under the hood by the database chain` \\\n",
    "`pip install sentence_transformers  # Possibly needed for Hugging Face embeddings` \\\n",
    "`pip install chromadb  # Vector database` \\\n",
    "\\\n",
    "**Note:** Depending on your environment, you can choose to install ctransformers with or without CUDA support based on your available hardware. \\\n",
    "\\\n",
    "By following these steps, you'll have the necessary components in place to successfully run the notebook. If you encounter any issues or have questions, refer to the README in the GitHub repository for further guidance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e55aee",
   "metadata": {},
   "source": [
    "## Getting the CodeLlama LLM\n",
    "\n",
    "We are going to use the \"Quantized\" version of CodeLlama 7B to be more memory optimized as we run it locally on the laptop. The default 7B model would take up 7x4 = 28 GB of memory. When we take the Q4 version, it is 4-bit or should take roughly 3.5 GB to load. However, when we run inference, it will expand to more than double the size. I observed that my RAM usage was going upto 9 GB sometime.\n",
    "\n",
    "We are also setting the context_length to 10000, which is really large. I could not find a better way as we are using prompt learning, so all the context (schemas, tables, queries) will need to be passed in the prompt for this to work. Let me know if there is a better way to do this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1, 'temperature': 0, 'context_length': 10000}\n",
    "#https://github.com/marella/ctransformers#config For config of CTransformers\n",
    "\n",
    "llm = CTransformers(model=\"TheBloke/CodeLlama-7B-Instruct-GGUF\", \n",
    "                    model_file=\"codellama-7b-instruct.Q4_K_M.gguf\",config=config, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb619a20",
   "metadata": {},
   "source": [
    "### Once the models are downloaded connect to the Database.\n",
    "\n",
    "LangChain uses SQLAlchemy under the hood to talk to and query database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "db = SQLDatabase.from_uri('mysql://dbuser:dbpwd@localhost:3306/sakila',\n",
    "        #include_tables=['customer', 'address', 'city', 'country'], # include only the tables you want to query. Reduces tokens.\n",
    "        sample_rows_in_table_info=3\n",
    "    )\n",
    "\n",
    "print(db.table_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba8a8f",
   "metadata": {},
   "source": [
    "### Setup the chain and run inference.\n",
    "\n",
    "In following cell we run the database chain 'as-is' or with zero shot training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, return_sql=False, use_query_checker=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "db_chain.run(\"How many customers are from district California?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct and run query: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d484f9",
   "metadata": {},
   "source": [
    "### Creating an example_prompt and an array of examples. 3 examples shared below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "        {\n",
    "            \"input\": \"How many customers are from district California?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            WHERE ad.district = 'California';\",\n",
    "            \"result\": \"[(9,)]\",\n",
    "            \"answer\": \"There are 9 customers from California\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How many customers are from city San Bernardino?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            JOIN city ci  ON ad.city_id = ci.city_id WHERE ci.city = 'San Bernardino';\",\n",
    "            \"result\": \"[(1,)]\",\n",
    "            \"answer\": \"There is 1 customer from San Bernardino\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How many customers are from country United States?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            JOIN city ci ON ad.city_id = ci.city_id JOIN country co ON ci.country_id = co.country_id \\\n",
    "            WHERE co.country = 'United States';\",\n",
    "            \"result\": \"[(36,)]\",\n",
    "            \"answer\": \"There are 36 customers from United States\",\n",
    "        },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"sql_cmd\", \"result\", \"answer\",],\n",
    "    template=\"\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {result}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "#print(example_prompt.format(**examples[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9aea4",
   "metadata": {},
   "source": [
    "### Vectorizing the examples shared above and storing them in a local Chroma vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7b924",
   "metadata": {},
   "source": [
    "### Setting up the Few Shot Prompt which will be passed on to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
    "\n",
    "#print(PROMPT_SUFFIX)\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_mysql_prompt,\n",
    "    suffix=PROMPT_SUFFIX, \n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"], #These variables are used in the prefix and suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582a9e0",
   "metadata": {},
   "source": [
    "### Setup the chain from LLM and run it\n",
    "\n",
    "In the prompt below I am showing how a complex query can be constructed using JOIN's. The same question does not get a response from CodeLlama when we run it with zero shot as the query is constructed across four tables (customer, address, city and country)\n",
    "\n",
    "The next prompt after that is the same as the first prompt we did with zero shot. This is just to check if there was any performance improvement. However, it appears that performance degraded a bit, although we got the LLM to use JOIN instead of a sub query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_chain = SQLDatabaseChain.from_llm(llm, db, prompt=few_shot_prompt, use_query_checker=True, \n",
    "                                        verbose=True, return_sql=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "local_chain.run(\"How many customers are from country Canada?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct query: {elapsed_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39360b5",
   "metadata": {},
   "source": [
    "### Rerunning the first prompt to check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f78faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "local_chain.run(\"How many customers are from district California?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct and run query: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76430a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
