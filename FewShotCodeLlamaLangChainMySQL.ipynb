{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a19e361",
   "metadata": {},
   "source": [
    "# Enhancing SQL Queries with Few-Shot Learning via CodeLlama and LangChain\n",
    "\n",
    "## Prerequisites for running this notebook\n",
    "For running this notebook, we will first need to meet a few prerequisites. You can fork the code from https://github.com/yernenip/CodeLlama-LangChain-MySql\n",
    "\n",
    "1. First, get your Docker container or MySQL up and running with Sakila DB installed on it. You can refer to the readme in above git repository to learn how to do this.\n",
    "2. Next, install all the packages required to run this notebook using pip install. These packages are\n",
    "\n",
    "* ctransformers (base transformers with no GPU acceleration, as I am running this locally on CPU)\n",
    "* ctransformers[cuda] (if you have cuda support, then this will provide CUDA GPU acceleration, install this instead)\n",
    "* langchain (core stuff)\n",
    "* sqlalchemy (database chain uses this under the hood)\n",
    "* sentence_transformers (possibly needed for hugging face embeddings?)\n",
    "* chromadb ( vector database)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d47e3",
   "metadata": {},
   "source": [
    "## Getting the CodeLlama LLM\n",
    "\n",
    "We are going to use the \"Quantized\" version of CodeLlama 7B to be more memory optimized as we run it locally on the laptop. The default 7B model would take up 7x4 = 28 GB of memory. When we take the Q4 version, it is 4-bit or should take roughly 3.5 GB to load. However, when we run inference, it will expand to almost double the size. So we end up consuming over 7 GB in RAM.\n",
    "\n",
    "We are also setting the context_length to 10000, which is really large. I could not find a better way as we are using prompt learning, so all the context (schemas, tables, queries) will need to be passed in the prompt for this to work. Let me know if there is a better way to do this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1, 'temperature': 0, 'context_length': 10000}\n",
    "#https://github.com/marella/ctransformers#config For config of CTransformers\n",
    "\n",
    "llm = CTransformers(model=\"TheBloke/CodeLlama-7B-Instruct-GGUF\", \n",
    "                    model_file=\"codellama-7b-instruct.Q4_K_M.gguf\",config=config, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc91c80",
   "metadata": {},
   "source": [
    "### Once the models are downloaded connect to the Database.\n",
    "\n",
    "LangChain uses SQLAlchemy under the hood to talk to and query database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "langchain.verbose = True\n",
    "\n",
    "db = SQLDatabase.from_uri('mysql://dbuser:dbpwd@localhost:3306/sakila',\n",
    "        #include_tables=['customer', 'address', 'city', 'country'], # include only the tables you want to query. Reduces tokens.\n",
    "        sample_rows_in_table_info=3\n",
    "    )\n",
    "\n",
    "print(db.table_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b297589",
   "metadata": {},
   "source": [
    "### Setup the chain and run inference.\n",
    "\n",
    "In following cell we run the database chain 'as-is' or with zero shot training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, return_sql=False, use_query_checker=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "db_chain.run(\"How many customers are from district California?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct and run query: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e90da",
   "metadata": {},
   "source": [
    "### Creating an example_prompt and an array of examples. 3 examples shared below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "        {\n",
    "            \"input\": \"How many customers are from district California?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            WHERE ad.district = 'California';\",\n",
    "            \"result\": \"[(9,)]\",\n",
    "            \"answer\": \"There are 9 customers from California\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How many customers are from city San Bernardino?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            JOIN city ci  ON ad.city_id = ci.city_id WHERE ci.city = 'San Bernardino';\",\n",
    "            \"result\": \"[(1,)]\",\n",
    "            \"answer\": \"There is 1 customer from San Bernardino\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How many customers are from country United States?\",\n",
    "            \"sql_cmd\": \"SELECT COUNT(*) FROM customer cu JOIN address ad ON cu.address_id = ad.address_id \\\n",
    "            JOIN city ci ON ad.city_id = ci.city_id JOIN country co ON ci.country_id = co.country_id \\\n",
    "            WHERE co.country = 'United States';\",\n",
    "            \"result\": \"[(36,)]\",\n",
    "            \"answer\": \"There are 36 customers from United States\",\n",
    "        },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"sql_cmd\", \"result\", \"answer\",],\n",
    "    template=\"\\nQuestion: {input}\\nSQLQuery: {sql_cmd}\\nSQLResult: {result}\\nAnswer: {answer}\",\n",
    ")\n",
    "\n",
    "#print(example_prompt.format(**examples[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd187f4",
   "metadata": {},
   "source": [
    "### Vectorizing the examples shared above and storing them in a local Chroma vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b3435",
   "metadata": {},
   "source": [
    "### Setting up the Few Shot Prompt which will be passed on to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
    "\n",
    "#print(PROMPT_SUFFIX)\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=_mysql_prompt,\n",
    "    suffix=PROMPT_SUFFIX, \n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"], #These variables are used in the prefix and suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141837a",
   "metadata": {},
   "source": [
    "### Setup the chain from LLM and run it\n",
    "\n",
    "In the prompt below I am showing how a complex query can be constructed using JOIN's. The same question does not get a response from CodeLlama when we run it with zero shot as the query is constructed across four tables (customer, address, city and country)\n",
    "\n",
    "The next prompt after that is the same as the first prompt we did with zero shot. This is just to check if there was any performance improvement. However, it appears that performance degraded a bit, although we got the LLM to use JOIN instead of a sub query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_chain = SQLDatabaseChain.from_llm(llm, db, prompt=few_shot_prompt, use_query_checker=True, \n",
    "                                        verbose=True, return_sql=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "local_chain.run(\"How many customers are from country Canada?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct query: {elapsed_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6ebbe",
   "metadata": {},
   "source": [
    "### Rerunning the first prompt to check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f78faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "local_chain.run(\"How many customers are from district California?\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f\"Time taken to construct and run query: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74865a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
